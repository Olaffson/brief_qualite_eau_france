name: CI

on:
  workflow_dispatch: {}
  # schedule:
  #   - cron: "30 5 * * 1"

jobs:
  run-ingest:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      AZURE_STORAGE_CONNECTION_STRING: ${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Show repo tree (debug)
        run: ls -la && echo "---" && ls -la notebooks

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: "notebooks/01_ingest_qualite_eau/requirements.txt" 

      - name: Install deps
        working-directory: notebooks/01_ingest_qualite_eau
        run: pip install -r requirements.txt

      - name: Ingest zip
        working-directory: notebooks/01_ingest_qualite_eau
        run: python 01_ingest_zip.py

      - name: Unzip
        working-directory: notebooks/01_ingest_qualite_eau
        run: python 02_unzip.py

      - name: Build yearly PLV Parquet
        working-directory: notebooks/01_ingest_qualite_eau
        run: python 03_build_parquet_plv.py

      - name: Build yearly RESULT Parquet
        working-directory: notebooks/01_ingest_qualite_eau
        run: python 04_build_parquet_result.py
        
  run-pipeline:
    needs: run-ingest
    runs-on: ubuntu-latest
    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

    steps:
      - name: Create / Update Pipeline Job in Databricks
        run: |
          JOB_NAME="pipeline_qualite_eau_bronze_silver"

          # V√©rifie si le job existe d√©j√†
          JOB_ID=$(curl -s -X GET \
            -H "Authorization: Bearer $DATABRICKS_TOKEN" \
            "$DATABRICKS_HOST/api/2.1/jobs/list" | jq -r ".jobs[] | select(.settings.name==\"$JOB_NAME\") | .job_id")

          if [ -z "$JOB_ID" ]; then
            echo "üÜï Job n'existe pas ‚Üí cr√©ation..."

            CREATE_RESPONSE=$(curl -s -X POST "$DATABRICKS_HOST/api/2.1/jobs/create" \
              -H "Authorization: Bearer $DATABRICKS_TOKEN" \
              -H "Content-Type: application/json" \
              -d '{
                "name": "'"$JOB_NAME"'",
                "tasks": [
                  {
                    "task_key": "bronze_step",
                    "existing_cluster_id": "1027-125758-8xtnixci",
                    "notebook_task": { "notebook_path": "/Workspace/Users/okotwica.ext@simplonformations.onmicrosoft.com/brief_qualite_eau_france/notebooks/02_bronze_import" }
                  },
                  {
                    "task_key": "silver_step",
                    "depends_on": [{ "task_key": "bronze_step" }],
                    "existing_cluster_id": "1027-125758-8xtnixci",
                    "notebook_task": { "notebook_path": "/Workspace/Users/okotwica.ext@simplonformations.onmicrosoft.com/brief_qualite_eau_france/notebooks/03_silver_transformation" }
                  }
                ]
              }')

            JOB_ID=$(echo "$CREATE_RESPONSE" | jq -r '.job_id')
            echo "‚úÖ Job cr√©√© : ID = $JOB_ID"
          else
            echo "‚úîÔ∏è Job d√©j√† existant : ID = $JOB_ID"
          fi

          echo "JOB_ID=$JOB_ID" >> $GITHUB_ENV

      - name: Run Databricks Pipeline Job
        run: |
          echo "üöÄ Lancement du pipeline Databricks (Job ID: $JOB_ID)..."
          RUN_ID=$(curl -s -X POST \
            -H "Authorization: Bearer $DATABRICKS_TOKEN" \
            "$DATABRICKS_HOST/api/2.1/jobs/run-now" \
            -d "{\"job_id\": $JOB_ID}" | jq -r '.run_id')

          echo "‚è≥ Pipeline en cours ‚Üí RUN_ID=$RUN_ID"

          # Optionnel : suivre l'ex√©cution
          sleep 20
          echo "üåê Consulter dans Databricks : $DATABRICKS_HOST/#job/$JOB_ID/run/$RUN_ID"
